%!TEX root = draft.tex
\vspace{-1mm}
\section{Related Work}
%\vspace{-1.5mm}
Many techniques for linearizability verification, e.g.,~\cite{conf/ppopp/VafeiadisHHS06,conf/cav/AmitRRSY07,conf/vmcai/Vafeiadis09,conf/tacas/AbdullaHHJR13}, are based on forward simulation arguments, and typically only work for libraries where the linearization point of every invocation of a method $m$ is fixed to a particular statement in the code of $m$. The works in~\cite{conf/cav/Vafeiadis10,Derrick2011,conf/cav/DragoiGH13,DBLP:conf/cav/ZhuPJ15} deal with \emph{external} linearization points where the action of an operation $k$ can be the linearization point of a concurrently executing operation $k'$. We say that the linearization point of $k'$ is external. This situation arises in read-only methods like the {\tt contains} method of an optimistic set~\cite{conf/podc/OHearnRVYY10}, libraries based on the elimination back-off scheme, e.g.,~\cite{conf/spaa/HendlerSY04}, or flat combining~\cite{DBLP:conf/spaa/HendlerIST10,DBLP:conf/podc/GorelikH13}. 
In these implementations, an operation can do an update on the shared state that becomes the linearization point of a concurrent read-only method (e.g., a {\tt contains} returning {\tt true} may be linearized when an {\tt add} method adds a new value to the shared state) or an operation may update the data structure on behalf of other concurrently executing operations (whose updates are published in the shared state). In all these cases, every linearization point can still be associated syntactically to a statement in the code of a method and doesn't depend on operations executed in the future (unlike $\mathit{HWQ}$ and $\mathit{TSS}$). However, identifying the set of operations for which such a statement is a linearization point can only be done by looking at the whole program state (the local states of all the active operations). This poses a problem in the context of compositional reasoning (where auxiliary variables are required), but still admits a forward simulation argument. For manual proofs, such implementations with external linearization points can still be defined as LTSs that produce $Lin$-complete traces and thus still fall in the class of implementations for which forward simulations are enough for proving refinement. These proof methods are not complete and they are not able to deal with implementations like $\mathit{HWQ}$ or $\mathit{TSS}$.

There also exist linearizability proof techniques based on backward simulations or alternatively, prophecy variables, e.g.,~\cite{phd/Vafeiadis08,DBLP:conf/cav/SchellhornWD12,DBLP:conf/pldi/LiangF13}. These works can deal with implementations where the linearization points are not fixed, but the proofs are conceptually more complex and less amenable to automation.

The works in~\cite{conf/concur/HenzingerSV13,DBLP:conf/icalp/BouajjaniEEH15} propose reductions of linearizability to assertion checking where the idea is to define finite-state automata that recognize violations of concurrent queues and stacks. These automata are simple enough in the case of queues and there is a proof of $\mathit{HWQ}$ based on this reduction~\cite{conf/concur/HenzingerSV13}. However, in the case of stacks, the automata become much more complicated and we are not aware of a proof for an implementation such as $\mathit{TSS}$ which is based on this reduction.
